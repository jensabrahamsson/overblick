{% extends "base.html" %}
{% block title %}AI Engine{% endblock %}

{% block content %}
<h2 class="step-heading">Choose Your AI Engine</h2>
<p class="step-description">
    Ã–verblick agents are powered by local LLMs. Choose between direct Ollama connection
    or the LLM Gateway (which adds priority queuing for multiple agents).
</p>

<form method="post" action="/step/3">
    {% set llm = state.llm or {} %}

    <div class="form-group">
        <label class="form-label">Provider</label>
        <div class="radio-cards">
            <label class="radio-card">
                <input type="radio" name="llm_provider" value="ollama"
                       {{ 'checked' if llm.get('llm_provider', 'ollama') == 'ollama' }}
                       onchange="document.getElementById('ollama-config').style.display='block';document.getElementById('gateway-config').style.display='none';">
                <div class="radio-card-title">Ollama (Local)</div>
                <div class="radio-card-desc">Direct connection to Ollama. Simple, no extra setup.</div>
            </label>
            <label class="radio-card">
                <input type="radio" name="llm_provider" value="gateway"
                       {{ 'checked' if llm.get('llm_provider') == 'gateway' }}
                       onchange="document.getElementById('gateway-config').style.display='block';document.getElementById('ollama-config').style.display='none';">
                <div class="radio-card-title">LLM Gateway</div>
                <div class="radio-card-desc">Priority queue for multiple agents. Better for production.</div>
            </label>
        </div>
    </div>

    <!-- Ollama config -->
    <div id="ollama-config" style="display: {{ 'none' if llm.get('llm_provider') == 'gateway' else 'block' }};">
        <div style="display: grid; grid-template-columns: 2fr 1fr; gap: var(--space-md);">
            <div class="form-group">
                <label class="form-label" for="ollama_host">Ollama Host</label>
                <input type="text" class="form-input" id="ollama_host" name="ollama_host"
                       value="{{ llm.get('ollama_host', '127.0.0.1') }}">
            </div>
            <div class="form-group">
                <label class="form-label" for="ollama_port">Port</label>
                <input type="number" class="form-input" id="ollama_port" name="ollama_port"
                       value="{{ llm.get('ollama_port', 11434) }}">
            </div>
        </div>

        <div class="form-group">
            <div style="display: flex; align-items: center; gap: var(--space-md);">
                <button type="button" class="btn btn-secondary btn-test"
                        hx-post="/test/ollama"
                        hx-include="#ollama_host, #ollama_port"
                        hx-target="#ollama-status">
                    Check Connection
                </button>
                <span id="ollama-status" class="test-result"></span>
            </div>
        </div>
    </div>

    <!-- Gateway config -->
    <div id="gateway-config" style="display: {{ 'block' if llm.get('llm_provider') == 'gateway' else 'none' }};">
        <div class="form-group">
            <label class="form-label" for="gateway_url">Gateway URL</label>
            <input type="text" class="form-input" id="gateway_url" name="gateway_url"
                   value="{{ llm.get('gateway_url', 'http://127.0.0.1:8200') }}">
        </div>
    </div>

    <!-- Common LLM settings -->
    <div class="form-group">
        <label class="form-label" for="model">Model</label>
        <span class="form-hint" id="model_hint">The LLM model to use (e.g. qwen3:8b, llama3:8b)</span>
        <input type="text" class="form-input" id="model" name="model"
               value="{{ llm.get('model', 'qwen3:8b') }}" list="model-suggestions"
               aria-describedby="model_hint">
        <datalist id="model-suggestions">
            <option value="qwen3:8b">
            <option value="qwen3:14b">
            <option value="llama3:8b">
            <option value="mistral:7b">
            <option value="gemma2:9b">
        </datalist>
    </div>

    <div class="form-group">
        <label class="form-label">
            Temperature: <span class="form-range-value" id="temp-value">{{ llm.get('default_temperature', 0.7) }}</span>
        </label>
        <span class="form-hint" id="temp_hint">Lower = more focused, Higher = more creative (0.0 - 2.0)</span>
        <input type="range" class="form-range" id="default_temperature" name="default_temperature"
               min="0" max="2" step="0.1" value="{{ llm.get('default_temperature', 0.7) }}"
               oninput="document.getElementById('temp-value').textContent = this.value"
               aria-describedby="temp_hint">
    </div>

    <div class="form-group">
        <label class="form-label" for="default_max_tokens">Max Tokens</label>
        <span class="form-hint" id="max_tokens_hint">Maximum response length (100 - 32000)</span>
        <input type="number" class="form-input" id="default_max_tokens" name="default_max_tokens"
               value="{{ llm.get('default_max_tokens', 2000) }}" min="100" max="32000"
               aria-describedby="max_tokens_hint">
    </div>

    <div class="btn-group">
        <a href="/step/2" class="btn btn-secondary">Back</a>
        <button type="submit" class="btn btn-primary">Next: Channels</button>
    </div>
</form>
{% endblock %}
